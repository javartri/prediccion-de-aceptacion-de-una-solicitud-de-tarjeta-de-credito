{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"banner/banner_proyecto.png\" style=\"width:1000px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b style=\"color:#152F6E\"> PREDICCIÓN DE ACEPTACIÓN DE UNA SOLICITUD DE TARJETA DE CRÉDITO </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:20px;font-family:verdana\">MOTIVACIÓN</h1>\n",
    "\n",
    "<p style=\"font-family:bold\">Antiguamente e incluso sigue existiendo en la actualidad las industrias bancarias reciben miles de solicitudes de solicitud de tarjeta de crédito. Revisar cada solicitud manualmente puede llevar demasiado tiempo y también ser es susceptible a errores humanos. Por ende con este proyecto pretendemos realizar un predictor automático de aprobación de tarjetas de crédito utilizando técnicas de aprendizaje automático.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook es basado en Sayak Paul's Datacamp Project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Primero, cargamos y vemos el conjunto de datos. Descubrimos que dado que estos datos son confidenciales, el contribuyente del conjunto de datos ha anonimizado los nombres de las funciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File datasets/crx.data does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mIOError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-be135e78e714>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# carga dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcc_apps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"datasets/crx.data\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# cc_apps2 = CC_data[:,:]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File datasets/crx.data does not exist"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# carga dataset\n",
    "cc_apps = pd.read_csv(\"dataset/cc_approvals.data\",header=None)\n",
    "\n",
    "# cc_apps2 = CC_data[:,:]\n",
    "cc_apps2 = cc_apps.copy()\n",
    "\n",
    "# Inspeccionar datos\n",
    "print(cc_apps.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">Renombrando las columnas</span>: \n",
    "Las columnas no estaban identificadas con fines de privacidad en una publicación de Ryan Kuhn muestra que las columnas probablemente sean Sexo, Edad, Deuda, Casado, Cliente bancario, Nivel educativo, Etnia, Años Empleado, Predeterminado, Empleado, CreditScore, DriversLicense, Citizen, ZipCode, Income y finalmente el ApprovalStatus. Entonces cambiamos el nombre de las columnas del DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_apps.columns = ['Gender', 'Age', 'Debt', 'Married',\n",
    "              'BankCustomer', 'EducationLevel', 'Ethnicity',\n",
    "              'YearsEmployed', 'PriorDefault',\n",
    "              'Employed', 'CreditScore', 'DriversLicense',\n",
    "              'Citizen', 'ZipCode', 'Income', 'ApprovalStatus']\n",
    "cc_apps2.columns = ['Gender', 'Age', 'Debt', 'Married',\n",
    "              'BankCustomer', 'EducationLevel', 'Ethnicity',\n",
    "              'YearsEmployed', 'PriorDefault',\n",
    "              'Employed', 'CreditScore', 'DriversLicense',\n",
    "              'Citizen', 'ZipCode', 'Income', 'ApprovalStatus']\n",
    "\n",
    "cc_apps.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspeccionando las solicitudes \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imprimir resumen de estadísticas\n",
    "cc_apps_description = cc_apps.describe()\n",
    "print(cc_apps_description)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# imprime informacion del dataframe\n",
    "cc_apps_info = cc_apps.info()\n",
    "print(cc_apps_info)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Inspeccionar valores perdidos en el conjunto de datos\n",
    "cc_apps.tail(17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver desde nuestro primer vistazo a los datos, el conjunto de datos tiene una combinación de características numéricas y no numéricas. Esto se puede solucionar con algo de preprocesamiento, pero antes de hacerlo, anlizamos un poco más sobre el conjunto de datos  para ver si habian otros problemas del conjunto de datos que debian corregirse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manejo de los valores faltantes (parte1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El conjunto de datos tiene valores faltantes. Los valores que faltan en el conjunto de datos están etiquetados con '?', Que se puede ver en la salida de la última celda.\n",
    "Se remplaza temporalmente estos signos de interrogación con NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Inspeccionar valores perdidos en el conjunto de datos\n",
    "print(cc_apps.isnull().values.sum())\n",
    "\n",
    "# remplaza '?'con NaN\n",
    "cc_apps.replace('?', np.NaN, inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "# Inspeccione los valores faltantes nuevamente\n",
    "cc_apps.tail(17)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convierte Age a numerico\n",
    "cc_apps[\"Age\"] = pd.to_numeric(cc_apps[\"Age\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manejo de los valores faltantes (parte2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para los valores perdidos se remplazan los valores faltantes con una estrategia llamada imputación media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputar los valores faltantes con imputación media\n",
    "cc_apps.fillna(cc_apps.mean(), inplace=True)\n",
    "\n",
    "# cuenta el numero de NaNs en el dataset y\n",
    "\n",
    "print(cc_apps.isnull().values.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_apps.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Descripción y distribución de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plotDistPlot(col):\n",
    "    \"\"\"Flexibly plot a univariate distribution of observation\"\"\"\n",
    "    sns.distplot(col)\n",
    "    plt.show()\n",
    "plotDistPlot(cc_apps['Age'])\n",
    "plotDistPlot(cc_apps['Debt'])\n",
    "plotDistPlot(cc_apps['YearsEmployed'])\n",
    "plotDistPlot(cc_apps['CreditScore'])\n",
    "plotDistPlot(cc_apps['Income'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#matriz de correlación\n",
    "corrmat = cc_apps.corr()\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "sns.heatmap(corrmat, vmax=.8, square=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatterplot\n",
    "sns.set()\n",
    "cols = ['Age', 'Income', 'CreditScore', 'Debt', 'YearsEmployed']\n",
    "sns.pairplot(cc_apps[cols], size = 2.5)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.countplot(data = cc_apps, x = 'ApprovalStatus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manejo de los valores faltantes (parte3)\n",
    "Se remplazan los valores faltantes con los valores más frecuentes,como están presentes en las columnas respectivas. Esta es una buena práctica cuando se trata de imputar valores faltantes para datos categóricos en general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_apps.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterar sobre cada columna de cc_apps\n",
    "print(cc_apps.info())\n",
    "for col in cc_apps.columns:\n",
    "    # Compruebe si la columna es de tipo de objeto\n",
    "    if cc_apps[col].dtypes == 'object':\n",
    "        # Imputar con el valor más frecuente\n",
    "        cc_apps[col] = cc_apps[col].fillna(cc_apps[col].value_counts().index[0])\n",
    "        # Para columnas numéricas, reemplace nan con la media.\n",
    "    else:\n",
    "        cc_apps[col] = cc_apps[col].fillna(cc_apps[col].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_apps.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#Instanciar LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Iterar sobre todos los valores de cada columna y extraer sus tipos\n",
    "for col in cc_apps.columns:\n",
    "    # Compare si el dtype es objcet\n",
    "    if cc_apps[col].dtype=='object':\n",
    "    # Use LabelEncoder para hacer la transformación numérica\n",
    "        cc_apps[col]=le.fit_transform(cc_apps[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cc_apps.tail(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelado y evaluación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Eliminando la característica que no son importante \n",
    "# Descarte las características 'DriversLicense', 'ZipCode' y convierte el DataFrame en una matriz NumPy\n",
    "cc_apps = cc_apps.drop(['DriversLicense', 'ZipCode'], axis=1)\n",
    "cc_apps = cc_apps.values\n",
    "\n",
    "# Crear una nueva variable para ingresar características y etiquetas\n",
    "# Separe las características y etiquetas en variables separadas\n",
    "X,y = cc_apps[:,0:13], cc_apps[:,13]\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#  Instancia MinMaxScaler y se usa para reescalar\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "rescaledX = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividiendo el conjunto de datos en train & test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(rescaledX,\n",
    "                                                    y,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ajustar un modelo de regresión logística al conjunto de trains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Instanciar un clasificador LogisticRegression con valores de parámetros predeterminados\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "#Ajuste el registro al conjunto de train set\n",
    "\n",
    "logreg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Haciendo predicciones y evaluando el rendimiento \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Usa logreg para predecir instancias del conjunto de prueba y almacenarlo\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Obtenga el puntaje de precisión del modelo logreg e imprímalo\n",
    "print(\"Accuracy of logistic regression classifier: \", logreg.score(X_test, y_test))\n",
    "\n",
    "# Imprime la matriz de confusión del modelo logreg\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividiendo el conjunto de datos en train & test sets\n",
    "X_train, X_test, y_train, Y_test = train_test_split(X,\n",
    "                                y,\n",
    "                                test_size=0.2,\n",
    "                                random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escalando los datos\n",
    "\n",
    "Dado que los datos varían de una columna a otra, podemos usar un escalador para reducir este efecto, porque algunos modelos son más sensibles a la variación entre los datos. Para hacer esto, importamos Scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaledX_train = scaler.fit_transform(X_train)\n",
    "rescaledX_test = scaler.transform(X_test)\n",
    "rescaledX = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Ajuste de  Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "rf = RandomForestClassifier(n_estimators=500)\n",
    "rf.fit(rescaledX_train, y_train)\n",
    "y_pred = rf.predict(rescaledX_test)\n",
    "print(\"Random Forest classifier has accuracy of: \", rf.score(rescaledX_test, Y_test))\n",
    "# Evalua la confusion_matrix\n",
    "confusion_matrix(Y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "importances = rf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rf.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Traza las características importantes del forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué atributo ha jugado el papel más importante en la decisión de solicitud de tarjeta de crédito?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_apps2= cc_apps2.drop(['ApprovalStatus'], axis=1)\n",
    "\n",
    "features = cc_apps2.columns\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimización de resultados: ajuste de hiperparámetros\n",
    "\n",
    "Podemos usar GridSearchCV para realizar ajustes de hiperparámetros en el Random Forest  y una cross validation para evitar el sobreajuste. El GridSearchCV realiza el cálculo utilizando todas las combinaciones entre la cuadrícula de parámetros.\n",
    "\n",
    "Hace una validación cruzada también."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso de redes neronales "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(rescaledX,\n",
    "                                                    y,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(552 ,input_dim=13),\n",
    "    keras.layers.Dense(200, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(200, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(200, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(2, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(), \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid searching, haciendo que el modelo funcione mejor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define la cuadrícula de valores para tol y max_iter\n",
    "tol = [0.01, 0.001, 0.0001]\n",
    "max_iter = [100, 150, 200]\n",
    "\n",
    "# Cree un diccionario donde tol y max_iter son claves y las listas de sus valores son valores correspondientes\n",
    "param_grid = dict(tol=tol, max_iter=max_iter)\n",
    "print(param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encontrar el mejor modelo de rendimiento\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancia GridSearchCV con los parámetros requeridos\n",
    "grid_model = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=5)\n",
    "\n",
    "# Fit data a grid_model\n",
    "grid_model_result = grid_model.fit(rescaledX, y)\n",
    "\n",
    "# resumen de resultados\n",
    "best_score, best_params = grid_model_result.best_score_,grid_model_result.best_params_\n",
    "print(\"Best: %f using %s\" % (best_score, best_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al construir este predictor de tarjeta de crédito, abordamos algunos de los pasos de preprocesamiento más conocidos, como el escalado, la codificación de etiquetas y la imputación de valores perdidos. Terminamos con algo de aprendizaje automático para predecir si la solicitud de una persona para una tarjeta de crédito sería aprobada o no, dada una informacion acerca de la persona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
